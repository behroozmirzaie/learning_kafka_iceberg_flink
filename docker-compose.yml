version: '3.8'

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.0
    container_name: zookeeper
    networks:
      - learning-net
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    image: confluentinc/cp-kafka:7.3.0
    container_name: kafka
    networks:
      - learning-net
    ports:
      - "9092:9092"
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,PLAINTEXT_INTERNAL://kafka:29092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'false'

  nessie:
    image: ghcr.io/projectnessie/nessie:0.78.0
    container_name: nessie
    networks:
      - learning-net
    ports:
      - "19120:19120"

  minio:
    image: minio/minio:RELEASE.2023.09.07T02-05-02Z
    container_name: minio
    networks:
      - learning-net
    environment:
      - MINIO_ROOT_USER=admin
      - MINIO_ROOT_PASSWORD=password
    ports:
      - "9000:9000"
      - "9001:9001"
    command: server /data --console-address ":9001"

  mc:
    image: minio/mc
    depends_on:
      - minio
    networks:
      - learning-net
    entrypoint: >
      /bin/sh -c "
      until (/usr/bin/mc config host add minio http://minio:9000 admin password) do echo '...waiting for minio' && sleep 1; done;
      /usr/bin/mc mb minio/warehouse;
      /usr/bin/mc policy set public minio/warehouse;
      exit 0;
      "

  spark-iceberg:
    image: tabulario/spark-iceberg
    container_name: spark-iceberg
    depends_on:
      mc:
        condition: service_completed_successfully
    networks:
      - learning-net
    environment:
      - SPARK_SQL_CATALOG_CATALOG=org.apache.iceberg.spark.SparkCatalog
      - SPARK_SQL_CATALOG_CATALOG_CATALOG__IMPL=org.apache.iceberg.nessie.NessieCatalog
      - SPARK_SQL_CATALOG_CATALOG_URI=http://nessie:19120/api/v1
      - SPARK_SQL_CATALOG_CATALOG_REFINMEMORY=true
      - SPARK_SQL_CATALOG_CATALOG_WAREHOUSE=s3a://warehouse
      - SPARK_HADOOP_FS_S3A_ENDPOINT=http://minio:9000
      - SPARK_HADOOP_FS_S3A_ACCESS_KEY=admin
      - SPARK_HADOOP_FS_S3A_SECRET_KEY=password
      - SPARK_HADOOP_FS_S3A_PATH__STYLE__ACCESS=true
      - SPARK_PACKAGES=org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.4.2,org.apache.hadoop:hadoop-aws:3.3.2,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.3_2.12:0.78.0

  flink:
    image: apache/flink:1.18.1-scala_2.12-java11
    container_name: flink
    networks:
      - learning-net
    ports:
      - "8081:8081"
    depends_on:
      - nessie
      - mc
    volumes:
      - ./flink-jars:/opt/flink/usrlib
    command: jobmanager
    environment:
      FLINK_PROPERTIES: "jobmanager.rpc.address: flink"
      # Add S3 support to Flink
      ENABLE_S3_HA: "true"

networks:
  learning-net:
